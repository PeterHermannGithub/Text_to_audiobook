#!/usr/bin/env python3
"""
LLM Integration Test Suite for HTTP Connection Pooling

This test suite validates the integration between HTTP connection pooling
and the existing LLM infrastructure components.

Integration Points Tested:
1. LLM Orchestrator with HTTP Connection Pooling
2. LLM Pool Manager with Connection Pooling  
3. Text Processing Pipeline Integration
4. Performance with Real LLM Endpoints (if available)
5. Fallback Behavior when Connection Pooling Disabled

Test Methodology:
- Tests actual LLM orchestrator with connection pooling enabled/disabled
- Validates connection pool statistics integration
- Tests error handling and fallback mechanisms
- Measures integration overhead and performance impact
"""

import asyncio
import logging
import os
import sys
import time
from typing import Dict, List, Optional, Any
import json

# Add the src directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LLMIntegrationTest:
    """Integration test suite for LLM components with HTTP connection pooling."""
    
    def __init__(self):
        self.tests_passed = 0
        self.tests_failed = 0
        self.integration_results = {}
        
    def run_all_tests(self):
        """Run all LLM integration tests."""
        logger.info("="*70)
        logger.info("LLM INTEGRATION TEST SUITE - HTTP CONNECTION POOLING")
        logger.info("="*70)
        
        # Test 1: LLM Orchestrator Integration
        self.test_llm_orchestrator_integration()
        
        # Test 2: LLM Pool Manager Integration
        self.test_llm_pool_manager_integration()
        
        # Test 3: Connection Pool Configuration Integration
        self.test_connection_pool_configuration()
        
        # Test 4: Error Handling Integration
        self.test_error_handling_integration()
        
        # Test 5: Performance Integration
        self.test_performance_integration()
        
        # Test 6: Fallback Behavior
        self.test_fallback_behavior()
        
        # Print final results
        self.print_integration_results()
        
        return self.tests_failed == 0
    
    def test_llm_orchestrator_integration(self):
        """Test LLM Orchestrator with HTTP connection pooling."""
        logger.info("\nğŸ¤– Testing LLM Orchestrator integration...")
        
        try:
            from attribution.llm.orchestrator import LLMOrchestrator
            from config import settings
            
            # Test with connection pooling enabled
            original_setting = settings.HTTP_POOL_ENABLED
            settings.HTTP_POOL_ENABLED = True
            
            config = {
                'engine': 'local',
                'local_model': 'deepseek-v2:16b'
            }
            
            orchestrator = LLMOrchestrator(config)
            
            # Verify connection pool manager is initialized
            assert orchestrator.http_pool_manager is not None, "HTTP pool manager should be initialized"
            
            # Test that orchestrator can create requests (without actually sending them)
            test_prompt = "Test prompt for integration"
            
            # Test prompt building (doesn't require LLM server)
            numbered_lines = ["1. Hello world", "2. How are you?"]
            prompt = orchestrator.build_classification_prompt(numbered_lines)
            assert prompt is not None and len(prompt) > 0, "Prompt should be generated"
            
            # Test cache integration
            cache_stats = orchestrator.get_cache_stats()
            assert isinstance(cache_stats, dict), "Cache stats should be available"
            
            # Test with connection pooling disabled
            settings.HTTP_POOL_ENABLED = False
            orchestrator_no_pool = LLMOrchestrator(config)
            assert orchestrator_no_pool.http_pool_manager is None, "HTTP pool manager should be None when disabled"
            
            # Restore original setting
            settings.HTTP_POOL_ENABLED = original_setting
            
            self.integration_results['orchestrator'] = {
                'pool_manager_initialized': True,
                'prompt_generation_working': True,
                'cache_integration_working': True,
                'fallback_working': True
            }
            
            self.tests_passed += 1
            logger.info("   âœ… LLM Orchestrator integration tests passed")
            
        except Exception as e:
            self.tests_failed += 1
            logger.error(f"   âŒ LLM Orchestrator integration tests failed: {e}")
    
    def test_llm_pool_manager_integration(self):
        """Test LLM Pool Manager with HTTP connection pooling."""
        logger.info("\nğŸ”„ Testing LLM Pool Manager integration...")
        
        try:
            from llm_pool.llm_pool_manager import LLMPoolManager
            from config import settings
            
            # Test with connection pooling enabled
            original_setting = settings.HTTP_POOL_ENABLED
            settings.HTTP_POOL_ENABLED = True
            
            pool_config = {
                'default_instances': [
                    {'host': 'localhost', 'port': 11434, 'model': 'deepseek-v2:16b'},
                    {'host': 'localhost', 'port': 11435, 'model': 'deepseek-v2:16b'}
                ],
                'max_concurrent_requests': 4,
                'request_timeout': 30.0
            }
            
            pool_manager = LLMPoolManager(pool_config)
            
            # Verify HTTP connection pool manager is initialized
            assert pool_manager.http_pool_manager is not None, "LLM Pool Manager should have HTTP pool manager"
            
            # Test pool status with connection pool stats
            status = pool_manager.get_pool_status()
            assert 'http_pool_stats' in status, "Pool status should include HTTP pool stats"
            assert isinstance(status['http_pool_stats'], dict), "HTTP pool stats should be a dictionary"
            
            # Test pool manager configuration
            assert len(pool_manager.instances) == 2, "Should have 2 configured instances"
            
            # Test connection pool configuration
            http_config = pool_manager.http_pool_config
            assert http_config is not None, "HTTP pool configuration should be available"
            
            # Test cleanup
            pool_manager.stop()
            
            # Test with connection pooling disabled
            settings.HTTP_POOL_ENABLED = False
            pool_manager_no_pool = LLMPoolManager(pool_config)
            assert pool_manager_no_pool.http_pool_manager is None, "Should not have HTTP pool manager when disabled"
            pool_manager_no_pool.stop()
            
            # Restore original setting
            settings.HTTP_POOL_ENABLED = original_setting
            
            self.integration_results['pool_manager'] = {
                'http_pool_integrated': True,
                'status_includes_pool_stats': True,
                'configuration_working': True,
                'cleanup_working': True
            }
            
            self.tests_passed += 1
            logger.info("   âœ… LLM Pool Manager integration tests passed")
            
        except Exception as e:
            self.tests_failed += 1
            logger.error(f"   âŒ LLM Pool Manager integration tests failed: {e}")
    
    def test_connection_pool_configuration(self):
        """Test connection pool configuration integration."""
        logger.info("\nâš™ï¸  Testing connection pool configuration integration...")
        
        try:
            from config import settings
            from llm_pool.http_pool_manager import ConnectionPoolConfig
            
            # Test configuration from settings
            config = ConnectionPoolConfig.from_settings()
            
            # Verify configuration attributes
            assert hasattr(config, 'max_pool_connections'), "Should have max_pool_connections"
            assert hasattr(config, 'connection_timeout'), "Should have connection_timeout"
            assert hasattr(config, 'circuit_breaker_enabled'), "Should have circuit_breaker_enabled"
            
            # Test configuration values are reasonable
            assert config.max_pool_connections > 0, "max_pool_connections should be positive"
            assert config.connection_timeout > 0, "connection_timeout should be positive"
            assert isinstance(config.circuit_breaker_enabled, bool), "circuit_breaker_enabled should be boolean"
            
            # Test that settings integration works
            original_pool_size = getattr(settings, 'HTTP_POOL_SIZE', 20)
            original_timeout = getattr(settings, 'HTTP_CONNECTION_TIMEOUT', 15.0)
            
            # Verify settings are properly loaded
            assert config.max_pool_size == original_pool_size, "Pool size should match settings"
            assert config.connection_timeout == original_timeout, "Timeout should match settings"
            
            self.integration_results['configuration'] = {
                'settings_integration_working': True,
                'config_attributes_present': True,
                'values_reasonable': True,
                'settings_loaded_correctly': True
            }
            
            self.tests_passed += 1
            logger.info("   âœ… Connection pool configuration tests passed")
            
        except Exception as e:
            self.tests_failed += 1
            logger.error(f"   âŒ Connection pool configuration tests failed: {e}")
    
    def test_error_handling_integration(self):
        """Test error handling integration across components."""
        logger.info("\nğŸ›¡ï¸  Testing error handling integration...")
        
        try:
            from attribution.llm.orchestrator import LLMOrchestrator
            from llm_pool.http_pool_manager import HTTPConnectionPoolManager, ConnectionPoolConfig
            from config import settings
            
            # Test with connection pooling enabled
            original_setting = settings.HTTP_POOL_ENABLED
            settings.HTTP_POOL_ENABLED = True
            
            # Test circuit breaker integration
            config = ConnectionPoolConfig(
                circuit_breaker_enabled=True,
                circuit_breaker_failure_threshold=2,
                circuit_breaker_recovery_timeout=0.1
            )
            
            pool_manager = HTTPConnectionPoolManager(config)
            circuit_breaker = pool_manager.circuit_breaker
            
            # Test that circuit breaker can be accessed and manipulated
            initial_state = circuit_breaker.get_state()
            assert initial_state is not None, "Circuit breaker should have a state"
            
            # Test error recording
            circuit_breaker.record_failure()
            failure_count = circuit_breaker.get_failure_count()
            assert failure_count > 0, "Failure count should increase"
            
            # Test success recording
            circuit_breaker.record_success()
            
            pool_manager.close()
            
            # Test LLM orchestrator error handling
            llm_config = {
                'engine': 'local',
                'local_model': 'deepseek-v2:16b'
            }
            
            orchestrator = LLMOrchestrator(llm_config)
            
            # Test that orchestrator handles connection pool errors gracefully
            # (This doesn't actually make requests, just tests the setup)
            assert orchestrator.http_pool_manager is not None, "Orchestrator should have pool manager"
            
            # Restore original setting
            settings.HTTP_POOL_ENABLED = original_setting
            
            self.integration_results['error_handling'] = {
                'circuit_breaker_accessible': True,
                'failure_recording_working': True,
                'success_recording_working': True,
                'orchestrator_error_handling': True
            }
            
            self.tests_passed += 1
            logger.info("   âœ… Error handling integration tests passed")
            
        except Exception as e:
            self.tests_failed += 1
            logger.error(f"   âŒ Error handling integration tests failed: {e}")
    
    def test_performance_integration(self):
        """Test performance integration across components."""
        logger.info("\nğŸš€ Testing performance integration...")
        
        try:
            from attribution.llm.orchestrator import LLMOrchestrator
            from llm_pool.http_pool_manager import HTTPConnectionPoolManager
            from config import settings
            
            # Test initialization performance
            original_setting = settings.HTTP_POOL_ENABLED
            settings.HTTP_POOL_ENABLED = True
            
            # Measure orchestrator initialization time with pooling
            start_time = time.time()
            config = {
                'engine': 'local',
                'local_model': 'deepseek-v2:16b'
            }
            orchestrator_with_pool = LLMOrchestrator(config)
            init_time_with_pool = time.time() - start_time
            
            # Measure orchestrator initialization time without pooling
            settings.HTTP_POOL_ENABLED = False
            start_time = time.time()
            orchestrator_without_pool = LLMOrchestrator(config)
            init_time_without_pool = time.time() - start_time
            
            # Test that initialization overhead is reasonable (less than 100ms difference)
            init_overhead = init_time_with_pool - init_time_without_pool
            assert init_overhead < 0.1, f"Initialization overhead should be minimal, got {init_overhead:.3f}s"
            
            # Test prompt generation performance (doesn't require actual LLM)
            numbered_lines = ["1. Hello world", "2. How are you?", "3. What's your name?"]
            
            # Test with pooling
            settings.HTTP_POOL_ENABLED = True
            start_time = time.time()
            for i in range(100):
                prompt = orchestrator_with_pool.build_classification_prompt(numbered_lines)
            prompt_time_with_pool = time.time() - start_time
            
            # Test without pooling
            settings.HTTP_POOL_ENABLED = False
            start_time = time.time()
            for i in range(100):
                prompt = orchestrator_without_pool.build_classification_prompt(numbered_lines)
            prompt_time_without_pool = time.time() - start_time
            
            # Prompt generation should not be significantly affected by pooling
            prompt_overhead = abs(prompt_time_with_pool - prompt_time_without_pool)
            assert prompt_overhead < 0.01, f"Prompt generation overhead should be minimal, got {prompt_overhead:.3f}s"
            
            # Restore original setting
            settings.HTTP_POOL_ENABLED = original_setting
            
            self.integration_results['performance'] = {
                'init_time_with_pool': init_time_with_pool,
                'init_time_without_pool': init_time_without_pool,
                'init_overhead': init_overhead,
                'prompt_time_with_pool': prompt_time_with_pool,
                'prompt_time_without_pool': prompt_time_without_pool,
                'prompt_overhead': prompt_overhead,
                'overhead_acceptable': init_overhead < 0.1 and prompt_overhead < 0.01
            }
            
            self.tests_passed += 1
            logger.info("   âœ… Performance integration tests passed")
            logger.info(f"   ğŸ“Š Initialization overhead: {init_overhead*1000:.1f}ms")
            logger.info(f"   ğŸ“Š Prompt generation overhead: {prompt_overhead*1000:.1f}ms")
            
        except Exception as e:
            self.tests_failed += 1
            logger.error(f"   âŒ Performance integration tests failed: {e}")
    
    def test_fallback_behavior(self):
        """Test fallback behavior when components are unavailable."""
        logger.info("\nğŸ”„ Testing fallback behavior...")
        
        try:
            from attribution.llm.orchestrator import LLMOrchestrator
            from config import settings
            
            # Test graceful fallback when HTTP pooling is disabled
            original_setting = settings.HTTP_POOL_ENABLED
            settings.HTTP_POOL_ENABLED = False
            
            config = {
                'engine': 'local',
                'local_model': 'deepseek-v2:16b'
            }
            
            orchestrator = LLMOrchestrator(config)
            
            # Verify that orchestrator works without connection pooling
            assert orchestrator.http_pool_manager is None, "HTTP pool manager should be None when disabled"
            
            # Test that all orchestrator functions still work
            numbered_lines = ["1. Hello world", "2. How are you?"]
            prompt = orchestrator.build_classification_prompt(numbered_lines)
            assert prompt is not None and len(prompt) > 0, "Prompt generation should work without pooling"
            
            # Test cache operations still work
            cache_stats = orchestrator.get_cache_stats()
            assert isinstance(cache_stats, dict), "Cache operations should work without pooling"
            
            # Test clear cache functionality
            orchestrator.clear_cache()
            
            # Test with pooling re-enabled
            settings.HTTP_POOL_ENABLED = True
            orchestrator_with_pool = LLMOrchestrator(config)
            assert orchestrator_with_pool.http_pool_manager is not None, "Pool manager should be available when enabled"
            
            # Restore original setting
            settings.HTTP_POOL_ENABLED = original_setting
            
            self.integration_results['fallback'] = {
                'graceful_degradation': True,
                'functions_work_without_pool': True,
                'cache_operations_work': True,
                'can_reenable_pooling': True
            }
            
            self.tests_passed += 1
            logger.info("   âœ… Fallback behavior tests passed")
            
        except Exception as e:
            self.tests_failed += 1
            logger.error(f"   âŒ Fallback behavior tests failed: {e}")
    
    def print_integration_results(self):
        """Print comprehensive integration test results."""
        logger.info("\n" + "="*70)
        logger.info("LLM INTEGRATION TEST RESULTS")
        logger.info("="*70)
        
        total_tests = self.tests_passed + self.tests_failed
        success_rate = (self.tests_passed / total_tests) * 100 if total_tests > 0 else 0
        
        logger.info(f"Total integration tests: {total_tests}")
        logger.info(f"Tests passed: {self.tests_passed}")
        logger.info(f"Tests failed: {self.tests_failed}")
        logger.info(f"Success rate: {success_rate:.1f}%")
        
        # Integration summary
        if self.tests_failed == 0:
            logger.info(f"\nğŸ‰ All LLM integration tests passed!")
            logger.info(f"âœ… HTTP connection pooling integrates seamlessly with LLM infrastructure!")
            
            logger.info(f"\nğŸ”§ Integration Features Validated:")
            logger.info(f"   âœ… LLM Orchestrator with HTTP connection pooling")
            logger.info(f"   âœ… LLM Pool Manager with connection pool statistics")
            logger.info(f"   âœ… Configuration integration from settings")
            logger.info(f"   âœ… Error handling and circuit breaker integration")
            logger.info(f"   âœ… Performance integration with minimal overhead")
            logger.info(f"   âœ… Graceful fallback when pooling disabled")
            
            if 'performance' in self.integration_results:
                perf = self.integration_results['performance']
                logger.info(f"\nğŸ“Š Performance Integration Results:")
                logger.info(f"   â€¢ Initialization overhead: {perf['init_overhead']*1000:.1f}ms")
                logger.info(f"   â€¢ Prompt generation overhead: {perf['prompt_overhead']*1000:.1f}ms")
                logger.info(f"   â€¢ Overall impact: Minimal and acceptable")
            
            logger.info(f"\nğŸ¯ Phase 3.1.6.5 - LLM Integration Testing: COMPLETE")
            logger.info(f"ğŸš€ Ready for production deployment with HTTP connection pooling!")
            
        else:
            logger.error(f"\nâŒ {self.tests_failed} integration tests failed")
            logger.error(f"LLM integration needs attention before production deployment")
        
        # Detailed results
        logger.info(f"\nğŸ“ˆ DETAILED INTEGRATION RESULTS:")
        for component, results in self.integration_results.items():
            logger.info(f"\nğŸ” {component.upper()}:")
            for key, value in results.items():
                if isinstance(value, bool):
                    status = "âœ…" if value else "âŒ"
                    logger.info(f"   {status} {key}: {value}")
                elif isinstance(value, float):
                    if 'time' in key:
                        logger.info(f"   ğŸ“Š {key}: {value*1000:.1f}ms")
                    else:
                        logger.info(f"   ğŸ“Š {key}: {value:.3f}")
                else:
                    logger.info(f"   ğŸ“Š {key}: {value}")
        
        return self.tests_failed == 0


def main():
    """Run the LLM integration test suite."""
    test_suite = LLMIntegrationTest()
    success = test_suite.run_all_tests()
    
    # Save integration results
    results_file = "llm_integration_results.json"
    with open(results_file, 'w') as f:
        json.dump(test_suite.integration_results, f, indent=2)
    
    logger.info(f"\nğŸ“„ Integration test results saved to: {results_file}")
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())